{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Giris\n",
        "Merhaba arkadaşlar,<br>\n",
        "Bu eğitimde, NumPy dizilerinden tf.data.Dataset'e veri yüklemeye dair bir örnek yapacağız.<br>\n",
        "\n",
        "**Nedir NumPy?**<br>\n",
        "NumPy (Numerical Python), bilimsel hesaplama ve veri analizi için kullanılan, Python programlama diline ait bir kütüphanedir. Temel amacı, büyük, çok boyutlu diziler (arrays) ve matrislerle çalışmayı kolaylaştırmak ve bu işlemleri hızlı hale getirmektir."
      ],
      "metadata": {
        "id": "SLUQTPLpsDSh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sJgHW0ZKO590"
      },
      "outputs": [],
      "source": [
        "# Kutuphanelerin Yuklenmesi\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## .npz dosyasından yükle"
      ],
      "metadata": {
        "id": "9j_QgPiytNOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_URL = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz'\n",
        "\n",
        "path = tf.keras.utils.get_file('mnist.npz', DATA_URL)\n",
        "with np.load(path) as data:\n",
        "  train_examples = data['x_train']\n",
        "  train_lables = data['y_train']\n",
        "  test_examples = data['x_test']\n",
        "  test_lables = data['y_test']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LH4YH9UEtF7q",
        "outputId": "228f7518-a3a4-4d21-ed39-79be9310ee31"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tf.data.Dataset ile NumPy dizilerini yükleyin\n",
        "tf.data.Dataset.from_tensor_slices fonksiyonunu kullanarak bir TensorFlow veri seti oluşturacağız."
      ],
      "metadata": {
        "id": "PYZcjKEAtyUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_lables))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_lables))"
      ],
      "metadata": {
        "id": "LBVVbNs5tqg5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use the datasets\n",
        "Hazırladığımız TensorFlow veri kümesini yapay zeka modelinde kullanacağız."
      ],
      "metadata": {
        "id": "nBYhI4KlwWfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Veri kumesinin egitim icin hazirlanmasi\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE_BUFFER_SIZE = 100\n",
        "\n",
        "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "iUA2YJ6Yvw2-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model olusturulmasi\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape = (28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.RMSprop(),\n",
        "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
        "              metrics = ['sparse_categorical_accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qpm8vwa_w9Ae",
        "outputId": "c7386798-b452-4ed1-ef3b-2632c17b0580"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_dataset, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69yA6FWyxl2m",
        "outputId": "22d024b8-57b1-4ee4-9690-1be21cf2af9f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 9.1479 - sparse_categorical_accuracy: 0.8246\n",
            "Epoch 2/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.6482 - sparse_categorical_accuracy: 0.9215\n",
            "Epoch 3/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 0.3969 - sparse_categorical_accuracy: 0.9425\n",
            "Epoch 4/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 0.3211 - sparse_categorical_accuracy: 0.9537\n",
            "Epoch 5/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 0.2753 - sparse_categorical_accuracy: 0.9591\n",
            "Epoch 6/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.2336 - sparse_categorical_accuracy: 0.9650\n",
            "Epoch 7/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.2235 - sparse_categorical_accuracy: 0.9682\n",
            "Epoch 8/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.1986 - sparse_categorical_accuracy: 0.9689\n",
            "Epoch 9/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.1970 - sparse_categorical_accuracy: 0.9724\n",
            "Epoch 10/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.1720 - sparse_categorical_accuracy: 0.9752\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a7c8be677c0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Degerlendirilmesi\n",
        "model.evaluate(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4jOB1KtxoTu",
        "outputId": "2ab1cab6-5d7e-47d9-e3fa-a60efb0bf11e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6658 - sparse_categorical_accuracy: 0.9526\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5610553622245789, 0.9571999907493591]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sonuc\n",
        "Kısa bir çalışma oldu. NumPy kütüphanesi içerisinde bulunan bir veri setinin nasıl indirileceğini ve TensorFlow veri setine nasıl dönüştürüleceğini inceledik. Son olarak, veri kümesi üzerinde bir eğitim gerçekleştirerek çalışmamızı tamamladım. Umarım bu çalışma sizin için faydalı olmuştur. İyi günler, iyi çalışmalar dilerim.\n",
        "\n",
        "Aşağıdaki Linklerden beni takip edebilir ve yapacağım çalışmalardan haberdar olabilirsiniz!<br>\n",
        "[Linkedin](https://www.linkedin.com/in/ihsancenkiz/)<br>\n",
        "[Github](https://github.com/ihsncnkz)<br>\n",
        "[Kaggle](https://www.kaggle.com/ihsncnkz)"
      ],
      "metadata": {
        "id": "BrZs8TY7yOrt"
      }
    }
  ]
}